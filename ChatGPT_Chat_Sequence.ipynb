{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Misbaxx99/-komomi-og-it/blob/main/ChatGPT_Chat_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chat sequence"
      ],
      "metadata": {
        "id": "MgpnjDz_ARFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install OpenAI components"
      ],
      "metadata": {
        "id": "bmZomqAqUet4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMZtZ_fFULYL",
        "outputId": "a187ca75-1532-426a-c490-7bb4bed903b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.50.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtwUjrS2VbqJ",
        "outputId": "7aa44af7-3d8c-43b8-9ed1-9106c4afb2cf"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mount Google Drive"
      ],
      "metadata": {
        "id": "LiVhUlZfVsRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERwSE0yEV4Rs",
        "outputId": "95227f3e-8870-4e9b-8db3-1f308a50a06d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load .env file"
      ],
      "metadata": {
        "id": "cc3ly5uAWIYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('drive/My Drive/Colab Notebooks/env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5ycZHwQWNan",
        "outputId": "154cb2c3-7c56-4d80-d77d-a4d58715943e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialize Open AI client"
      ],
      "metadata": {
        "id": "uWB5Q5NqXCYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI as openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.api_key\n",
        "client = openai()\n"
      ],
      "metadata": {
        "id": "NRygMPIVXDzp"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(openai.api_key)"
      ],
      "metadata": {
        "id": "VwCSzYY9s2ue",
        "outputId": "5cfd2af5-1e55-4b1f-d211-6493d436157d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Various variables and settings"
      ],
      "metadata": {
        "id": "rerQJqKxX_i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gptModel=\"gpt-4o-mini\"\n",
        "chatCompletionChoices=1\n",
        "samplingTemperature=None\n",
        "maxCompletionTokens=100\n",
        "nucleusSampling=None"
      ],
      "metadata": {
        "id": "WmrMIHu-ZwNb"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1st prompt"
      ],
      "metadata": {
        "id": "A4nb_Yj2Z_1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "InitialSystemContent = \"You are a helpful teacher.\"\n",
        "#InitialSystemContent = \"You are an experienced sales copywriter. The name of my business is Future Mind Consulting, and we sell software development. Write a 200-word email that will persuade anyone who reads it to become a customer.\"\n"
      ],
      "metadata": {
        "id": "qGt6k0aqYxCt"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstUserContent = \"Are there other measures than time complexity for an algorithm?\"\n",
        "#FirstUserContent = \"Serious\""
      ],
      "metadata": {
        "id": "3Q13yuwTY4zt"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "WL5KOl_KZUFB"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgSDj-dyaVNe",
        "outputId": "31c89c69-6234-475e-eccc-432d266dd42c"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "EeCD7AUFZlrV"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FirstAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something:  FirstAssistantContent = \"Yes, there are other measures besides time complexfor an algorithm, such as space complexity.\"\n"
      ],
      "metadata": {
        "id": "aSHZU_1mbPJu"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(FirstAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ppulUHVbb1W",
        "outputId": "0b6665d4-0043-41cd-d9f2-8a0cfdb93273"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, in addition to time complexity, there are several other important measures and factors to consider when evaluating the performance and efficiency of an algorithm. Here are some of them:\n",
            "\n",
            "1. **Space Complexity**: This measures the amount of memory space required by an algorithm as a function of the input size. It is important to consider not just the amount of space needed for the input data but also the additional space required for variables, function call stacks, and dynamic memory allocations.\n",
            "\n",
            "2. **I/O Complexity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2nd prompt"
      ],
      "metadata": {
        "id": "RFqIPlLuflNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SecondUserContent = \"What is it?\"\n",
        "#SecondUserContent = \"It is for medical device industry\""
      ],
      "metadata": {
        "id": "r_RsKgpPfqEq"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "ZvDPMcZegS4h"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MNGcNulhYNy",
        "outputId": "9cac4652-c80f-4ed2-b86a-d2c6cac86cd8"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}, {'role': 'assistant', 'content': 'Yes, in addition to time complexity, there are several other important measures and factors to consider when evaluating the performance and efficiency of an algorithm. Here are some of them:\\n\\n1. **Space Complexity**: This measures the amount of memory space required by an algorithm as a function of the input size. It is important to consider not just the amount of space needed for the input data but also the additional space required for variables, function call stacks, and dynamic memory allocations.\\n\\n2. **I/O Complexity'}, {'role': 'user', 'content': 'What is it?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "XCQaA_8chjk-"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SecondAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something: SecondAssistantContent = \"<something>\""
      ],
      "metadata": {
        "id": "io-1QG3jhq_l"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(SecondAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJeh1LFthwQk",
        "outputId": "695df03d-5f63-4bc6-ddd6-a0b0f59fd858"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I/O complexity refers to the efficiency of an algorithm regarding input and output operations, particularly how well it handles data transfers between different storage systems (like disk and memory) or over networks. Here are some key points regarding I/O complexity:\n",
            "\n",
            "1. **Input/Output Operations**: These involve reading data from or writing data to storage devices. Such operations are often significantly slower than in-memory operations, especially when dealing with large datasets.\n",
            "\n",
            "2. **Cost of I/O Operations**: I/O complexity considers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3rd prompt"
      ],
      "metadata": {
        "id": "n1OwZfAhkuBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ThirdUserContent = \"<some content>\""
      ],
      "metadata": {
        "id": "oJLEhjbSk8gE"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": SecondAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": ThirdUserContent,\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "IgcFKH1YlOu4"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chatMessages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUY_FAiblngZ",
        "outputId": "9935cf27-54b7-4b88-c80c-b206de6dbfe0"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful teacher.'}, {'role': 'user', 'content': 'Are there other measures than time complexity for an algorithm?'}, {'role': 'assistant', 'content': 'Yes, in addition to time complexity, there are several other important measures and factors to consider when evaluating the performance and efficiency of an algorithm. Here are some of them:\\n\\n1. **Space Complexity**: This measures the amount of memory space required by an algorithm as a function of the input size. It is important to consider not just the amount of space needed for the input data but also the additional space required for variables, function call stacks, and dynamic memory allocations.\\n\\n2. **I/O Complexity'}, {'role': 'user', 'content': 'What is it?'}, {'role': 'assistant', 'content': 'I/O complexity refers to the efficiency of an algorithm regarding input and output operations, particularly how well it handles data transfers between different storage systems (like disk and memory) or over networks. Here are some key points regarding I/O complexity:\\n\\n1. **Input/Output Operations**: These involve reading data from or writing data to storage devices. Such operations are often significantly slower than in-memory operations, especially when dealing with large datasets.\\n\\n2. **Cost of I/O Operations**: I/O complexity considers'}, {'role': 'user', 'content': '<some content>'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature,max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)"
      ],
      "metadata": {
        "id": "dxvc-yxCl1--"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ThirdAssistantContent = response.choices[0].message.content\n",
        "#Force assitant message to be something: ThirdAssistantContent = \"<something>\""
      ],
      "metadata": {
        "id": "qrluUrmfmC7I"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ThirdAssistantContent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFjLzydvmOmb",
        "outputId": "37a2cf0b-f321-45bc-fa40-9ad4d3d2805a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems your message was cut off or incomplete. If you meant to ask something specific or provide additional context regarding \"I/O complexity\" or another topic, please feel free to elaborate, and I'll be happy to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises"
      ],
      "metadata": {
        "id": "p1fwf3DB_lOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answering the questions below you may benefit from these references:  \n",
        "\n",
        "\n",
        "1.   Chat completion parameters : https://platform.openai.com/docs/api-reference/chat/create\n",
        "2.   ChatGPT Prompts Library: https://gptbot.io/chatgpt-prompts/\n",
        "3.   Models overview: https://platform.openai.com/docs/models\n",
        "\n"
      ],
      "metadata": {
        "id": "XUSB9DnTBki3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Establish your own chat sequence with max 3. prompts (unless you extend the notebook with additional prompts). You may start with a prompt from the  'ChatGPT Prompts Library' as mentioned above.  \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "bhE77NaXAwzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Establish your own chat sequence with max 3. prompts (unless you extend the notebook with additional prompts). You may start with a prompt from the 'ChatGPT Prompts Library' as mentioned above.\n",
        "\n",
        "# ###Various variables and settings\n",
        "gptModel=\"gpt-4o-mini\"\n",
        "chatCompletionChoices=1\n",
        "samplingTemperature=None\n",
        "maxCompletionTokens=100\n",
        "nucleusSampling=None\n",
        "\n",
        "# ###1st prompt\n",
        "InitialSystemContent = \"You are a creative story writer.\"\n",
        "FirstUserContent = \"Write a short story about a talking cat who is a detective.\"\n",
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "]\n",
        "print(chatMessages)\n",
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)\n",
        "FirstAssistantContent = response.choices[0].message.content\n",
        "print(FirstAssistantContent)\n",
        "\n",
        "# ###2nd prompt\n",
        "SecondUserContent = \"What is the cat's name and what is his favorite food?\"\n",
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "]\n",
        "print(chatMessages)\n",
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature, max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)\n",
        "SecondAssistantContent = response.choices[0].message.content\n",
        "print(SecondAssistantContent)\n",
        "\n",
        "\n",
        "# ###3rd prompt\n",
        "ThirdUserContent = \"Describe the cat's personality.\"\n",
        "chatMessages=[\n",
        "    {\n",
        "        \"role\": \"system\", \"content\": InitialSystemContent\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": FirstUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": FirstAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": SecondUserContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": SecondAssistantContent,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": ThirdUserContent,\n",
        "    },\n",
        "]\n",
        "print(chatMessages)\n",
        "response = client.chat.completions.create(model=gptModel, messages=chatMessages, n=chatCompletionChoices, temperature=samplingTemperature,max_completion_tokens=maxCompletionTokens, top_p=nucleusSampling)\n",
        "ThirdAssistantContent = response.choices[0].message.content\n",
        "print(ThirdAssistantContent)\n"
      ],
      "metadata": {
        "id": "Tevp0w_cwT3W",
        "outputId": "5427b6f2-8908-4b1f-b475-aa4a663f1fb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a creative story writer.'}, {'role': 'user', 'content': 'Write a short story about a talking cat who is a detective.'}]\n",
            "### The Case of the Missing Goldfish\n",
            "\n",
            "In the quaint town of Willow Creek, where sunshine filtered through the leaves and laughter danced in the air, lived a clever detective unlike any other. His name was Inspector Whiskers, a sleek black cat with emerald-green eyes that seemed to see through walls and around corners. As a distinguished member of the Willow Creek Detective Agency, Inspector Whiskers took pride in solving the town's quirkiest mysteries.\n",
            "\n",
            "One sunny afternoon, as Inspector Whiskers\n",
            "[{'role': 'system', 'content': 'You are a creative story writer.'}, {'role': 'user', 'content': 'Write a short story about a talking cat who is a detective.'}, {'role': 'assistant', 'content': \"### The Case of the Missing Goldfish\\n\\nIn the quaint town of Willow Creek, where sunshine filtered through the leaves and laughter danced in the air, lived a clever detective unlike any other. His name was Inspector Whiskers, a sleek black cat with emerald-green eyes that seemed to see through walls and around corners. As a distinguished member of the Willow Creek Detective Agency, Inspector Whiskers took pride in solving the town's quirkiest mysteries.\\n\\nOne sunny afternoon, as Inspector Whiskers\"}, {'role': 'user', 'content': \"What is the cat's name and what is his favorite food?\"}]\n",
            "The cat's name is Inspector Whiskers, and his favorite food is tuna pâté, preferably served in a fine china bowl that his owner, Mrs. Thompson, reserved just for him. He regarded it as a gourmet delicacy, often savoring each bite as if he were at a five-star restaurant.\n",
            "[{'role': 'system', 'content': 'You are a creative story writer.'}, {'role': 'user', 'content': 'Write a short story about a talking cat who is a detective.'}, {'role': 'assistant', 'content': \"### The Case of the Missing Goldfish\\n\\nIn the quaint town of Willow Creek, where sunshine filtered through the leaves and laughter danced in the air, lived a clever detective unlike any other. His name was Inspector Whiskers, a sleek black cat with emerald-green eyes that seemed to see through walls and around corners. As a distinguished member of the Willow Creek Detective Agency, Inspector Whiskers took pride in solving the town's quirkiest mysteries.\\n\\nOne sunny afternoon, as Inspector Whiskers\"}, {'role': 'user', 'content': \"What is the cat's name and what is his favorite food?\"}, {'role': 'assistant', 'content': \"The cat's name is Inspector Whiskers, and his favorite food is tuna pâté, preferably served in a fine china bowl that his owner, Mrs. Thompson, reserved just for him. He regarded it as a gourmet delicacy, often savoring each bite as if he were at a five-star restaurant.\"}, {'role': 'user', 'content': \"Describe the cat's personality.\"}]\n",
            "Inspector Whiskers had a personality as vibrant and multifaceted as the cases he solved. He was astute and observant, with an uncanny ability to pick up on even the slightest details that others overlooked. His intelligence was matched only by his curiosity; he often found himself poking his nose into places where it didn’t belong, driven by a deep-seated desire to understand the world around him.\n",
            "\n",
            "Though he could be serious and focused when on a case, Inspector Whiskers had a playful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YlV_25_7KDu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Does it make any difference, if you force assistants 'content' to be somethings else, than that received in the previous reply from ChatGPT. Technically you can do this removing one or more '#Force assitant message to be something:' and specify something else.    \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "mXU-PEtZA0Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Does it make any difference, if you force assistants 'content' to be somethings else, than that received in the previous reply from ChatGPT. Technically you can do this removing one or more '#Force assitant message to be something:' and specify something else.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: Yes, it makes a significant difference if you force the assistant's content to be something else than what ChatGPT generated.\n",
        "\n",
        "# When you force the assistant's content, you are essentially overriding the model's response and providing your own input for the subsequent prompts in the chat sequence. This can lead to a few key consequences:\n",
        "\n",
        "# 1. Contextual Disconnect: If you force the assistant's content to be irrelevant or inconsistent with the previous conversation, it can confuse the model and lead to responses that are less coherent or accurate.\n",
        "\n",
        "# 2. Altered Conversation Flow: Forcing the assistant's content can change the direction of the conversation and lead it down a path that might not be aligned with the user's original intent.\n",
        "\n",
        "# 3. Potential Bias: If you force the assistant's content to reflect a particular viewpoint or perspective, you can introduce bias into the conversation, which can lead to skewed or limited responses.\n",
        "\n",
        "# In summary, while you can technically override the model's responses, it's generally best to allow ChatGPT to generate its responses naturally. This allows the model to learn from the context of the conversation and provide the most relevant and coherent answers possible.\n",
        "\n",
        "# If you want to influence the conversation or steer it in a specific direction, it's recommended to do so by crafting your prompts thoughtfully and providing clear instructions, rather than forcing the model's responses.\n"
      ],
      "metadata": {
        "id": "igIX9hRSwle5"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the 'model' parameter specified by the 'gptModel' variable.      \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "m0fNJEjSG5Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'model' parameter specified by the 'gptModel' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "# The 'model' parameter specified by the 'gptModel' variable determines which language model OpenAI's API will use to generate the response for your chat sequence.\n",
        "\n",
        "# In this notebook, 'gptModel' is set to \"gpt-4o-mini\". This refers to a specific version or type of the GPT (Generative Pre-trained Transformer) language model.\n",
        "\n",
        "# Different models have varying capabilities, strengths, and weaknesses, such as:\n",
        "\n",
        "# * **Size and training data:** Larger models have access to more training data and can potentially generate more comprehensive and sophisticated responses.\n",
        "# * **Performance:** Some models are optimized for specific tasks, such as text generation, translation, or code completion.\n",
        "# * **Cost:** Using different models might have varying costs associated with API usage.\n",
        "\n",
        "# By selecting a specific model in the 'gptModel' variable, you are choosing which language model's capabilities you want to leverage for your chat interaction. In this case, the \"gpt-4o-mini\" model is selected, which is likely a smaller and potentially more cost-effective variant of a larger model like GPT-4.\n",
        "\n",
        "# Refer to OpenAI's documentation on models (e.g., the \"Models overview\" link in the notebook) for detailed information about each specific model's features and capabilities, and to make informed decisions about selecting the most suitable model for your needs.\n"
      ],
      "metadata": {
        "id": "-v-g8cOHyCrZ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the 'messages' parameter specified by the 'chatMessages' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "MnyEfPT3HkRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'messages' parameter specified by the 'chatMessages' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: The 'messages' parameter specified by the 'chatMessages' variable is a list of dictionaries that represents the conversation history for the chat completion API.\n",
        "\n",
        "# Each dictionary within the 'chatMessages' list represents a single message in the conversation, and it has two key-value pairs:\n",
        "\n",
        "# 1. role: Specifies the role of the speaker, which can be either \"system,\" \"user,\" or \"assistant.\"\n",
        "\n",
        "#   - \"system\": Defines the overall persona or behavior of the assistant. For example, it can be set to \"You are a helpful assistant.\"\n",
        "#   - \"user\": Represents a message from the user.\n",
        "#   - \"assistant\": Represents a message from the AI assistant.\n",
        "\n",
        "# 2. content: Contains the actual text content of the message.\n",
        "\n",
        "\n",
        "# By providing a list of messages, you essentially reconstruct the context of the conversation for the language model. The model then uses this context to generate a response that is relevant to the conversation history.\n",
        "\n",
        "# For example, in the provided code, the 'chatMessages' list includes:\n",
        "\n",
        "# - A system message that defines the assistant as a creative story writer.\n",
        "# - A user message that asks for a short story about a talking cat detective.\n",
        "\n",
        "# When you send this list of messages to the OpenAI API through the 'messages' parameter, you're providing the necessary context for the model to generate a story about a detective cat.\n",
        "# The model then takes into account the conversation history and generates a response that is appropriate for the context.\n"
      ],
      "metadata": {
        "id": "IfWl9o9mySiL"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain the 'n' parameter specified by the 'chatCompletionChoices' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "W0ipFRpcH4-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'n' parameter specified by the 'chatCompletionChoices' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: The 'n' parameter specified by the 'chatCompletionChoices' variable controls the number of chat completion choices that the API should generate for each prompt.\n",
        "\n",
        "# In the provided code, 'chatCompletionChoices' is set to 1, which means that the API will generate only one completion for each prompt.\n",
        "# If you increase the value of 'n', for example to 3, the API will generate three different completions for the same prompt.\n",
        "# This can be helpful if you want to explore different possible responses or create a more diverse output.\n",
        "\n",
        "# Each completion will be a separate element within the 'choices' array in the API response. You can then access and process each of the completions independently.\n",
        "\n",
        "# For example, if 'n' is set to 3, the response might look like this:\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "MwABX0Ayyaj7"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the 'temperature' parameter specified by the 'samplingTemperature' variable.      \n",
        "Answer: ?"
      ],
      "metadata": {
        "id": "Q2RxcWOzIVM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'temperature' parameter specified by the 'samplingTemperature' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: The 'temperature' parameter specified by the 'samplingTemperature' variable controls the randomness of the generated text.\n",
        "\n",
        "# In essence, it determines how creative or predictable the model's response will be.\n",
        "\n",
        "# Here's a breakdown:\n",
        "\n",
        "# - Temperature = 0: The model will always choose the most likely next word, resulting in deterministic and predictable responses. It is useful for tasks where you need highly accurate and consistent results.\n",
        "\n",
        "# - Temperature > 0: The model will introduce some randomness in the word selection process. A higher temperature will lead to more diverse and creative responses, but it might also introduce some inconsistencies or nonsensical outputs.\n",
        "\n",
        "# - Higher temperature values: Increase the likelihood of generating less probable words and phrases, leading to more creative and surprising outputs. However, it also increases the risk of generating nonsensical or irrelevant content.\n",
        "\n",
        "# In the provided code, 'samplingTemperature' is set to None, which implies that the default temperature value will be used. The default value for temperature is likely 1.0, which introduces some randomness in the output.\n",
        "\n",
        "# By adjusting the temperature parameter, you can control the balance between creativity and predictability in your chat completion responses. For tasks that require highly accurate and consistent outputs, you might want to set a lower temperature. For tasks that require more creative and diverse outputs, you might want to set a higher temperature.\n"
      ],
      "metadata": {
        "id": "6n414fNTzT9w"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain the 'max_completion_tokens' parameter specified by the 'maxCompletionTokens' variable.      \n",
        "Answer: ?  "
      ],
      "metadata": {
        "id": "MwNggaKXIrFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'max_completion_tokens' parameter specified by the 'maxCompletionTokens' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: The 'max_completion_tokens' parameter specified by the 'maxCompletionTokens' variable limits the maximum number of tokens that the API should generate in its response.\n",
        "\n",
        "# A token can be a word, a part of a word, or a punctuation mark. Essentially, it's a unit of text.\n",
        "# By setting a limit on the number of tokens, you can control the length of the generated response and prevent it from becoming excessively long or exceeding the maximum allowed token limit for the API.\n",
        "\n",
        "# In the provided code, 'maxCompletionTokens' is set to 100, which means that the API will generate a response with a maximum of 100 tokens.\n",
        "# If the model generates a response that is longer than the specified limit, it will be truncated at the maximum allowed number of tokens.\n",
        "\n",
        "# Setting an appropriate value for 'max_completion_tokens' can help ensure that your responses are concise and within the desired length range.\n",
        "# It is also important to consider the context of your conversation and the potential length of the generated response when determining the maximum number of tokens you want to allow.\n"
      ],
      "metadata": {
        "id": "E0hFlye1zX_q"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Explain the 'top_p' parameter specified by the 'nucleusSampling' variable.      \n",
        "Answer: ?    "
      ],
      "metadata": {
        "id": "QW7wpSdKI_ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Explain the 'top_p' parameter specified by the 'nucleusSampling' variable.\n",
        "# Answer: ?\n",
        "\n",
        "# Answer: The 'top_p' parameter specified by the 'nucleusSampling' variable is another way to control the randomness of the generated text, alongside the 'temperature' parameter. It's also known as nucleus sampling.\n",
        "\n",
        "# Instead of sampling from all possible next words, nucleus sampling focuses on a subset of the most probable words. It considers the cumulative probability of the predicted words and only samples from those words whose cumulative probability is less than or equal to 'top_p'.\n",
        "\n",
        "# Here's how it works:\n",
        "\n",
        "# 1. The model calculates the probability distribution for the next word.\n",
        "# 2. It sorts the words in descending order of their probabilities.\n",
        "# 3. It selects the top words until their cumulative probability reaches the specified 'top_p' value.\n",
        "# 4. It then samples the next word from only this selected subset of words.\n",
        "\n",
        "# For example, if 'top_p' is set to 0.9, the model will consider the top words whose cumulative probability is less than or equal to 0.9. This means it will only sample from the most probable words, reducing the chance of generating less likely and potentially nonsensical words.\n",
        "\n",
        "# By adjusting 'top_p', you can control the diversity and quality of the generated text. A lower 'top_p' value will lead to more focused and predictable text, while a higher value will allow for more creativity and diversity.\n",
        "\n",
        "# In the provided code, 'nucleusSampling' is set to None, which implies that the default value for 'top_p' will be used. The default value is typically 1.0, which means that all possible words are considered for sampling.\n",
        "\n",
        "# By using 'top_p' alongside 'temperature', you can further fine-tune the output of your chat completion and achieve the desired balance between creativity, relevance, and coherence.\n"
      ],
      "metadata": {
        "id": "WjYHATaCzbpR"
      },
      "execution_count": 126,
      "outputs": []
    }
  ]
}